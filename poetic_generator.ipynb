{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQTZ/V9U8nMTiUkPflm9Hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YashiGarg016/Poetic-Generator/blob/main/poetic_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXuxnKtFLgyR"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "CaN7B34DobEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(filepath, 'rb').read().decode(encoding='utf-8').lower()"
      ],
      "metadata": {
        "id": "bZNzlq5fobJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text[300000:800000]"
      ],
      "metadata": {
        "id": "Z5QzUA9gpbpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "characters = sorted(set(text))"
      ],
      "metadata": {
        "id": "e887odV0pbv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_index = dict((c, i) for i, c in enumerate(characters))\n",
        "index_to_char = dict((i, c) for i, c in enumerate(characters))"
      ],
      "metadata": {
        "id": "M99jorlWpb0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LENGTH = 40\n",
        "STEP_SIZE = 3"
      ],
      "metadata": {
        "id": "Md5jjWyMCiY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "next_characters = []"
      ],
      "metadata": {
        "id": "tvIm2e2fobM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(text) - SEQ_LENGTH, STEP_SIZE):\n",
        "    sentences.append(text[i: i + SEQ_LENGTH])\n",
        "    next_characters.append(text[i + SEQ_LENGTH])"
      ],
      "metadata": {
        "id": "h92bD-vNDJTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.zeros((len(sentences), SEQ_LENGTH, len(characters)), dtype=bool)\n",
        "y = np.zeros((len(sentences), len(characters)), dtype=bool)"
      ],
      "metadata": {
        "id": "4zV8_sQiDJWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(sentences):\n",
        "    for t, character in enumerate(sentence):\n",
        "        x[i, t, char_to_index[character]] = 1\n",
        "    y[i, char_to_index[next_characters[i]]] = 1"
      ],
      "metadata": {
        "id": "M7918kJqCibT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " model = Sequential()\n",
        " model.add(LSTM(128, input_shape=(SEQ_LENGTH, len(characters))))\n",
        " model.add(Dense(len(characters)))\n",
        " model.add(Activation('softmax'))"
      ],
      "metadata": {
        "id": "7U1O2soICie3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(learning_rate=0.01))"
      ],
      "metadata": {
        "id": "sNXFy5Wefh2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x, y, batch_size = 256, epochs = 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4r89yCatfh5T",
        "outputId": "26377672-d7a4-484d-896e-5df8fff7a713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 191ms/step - loss: 2.4814\n",
            "Epoch 2/4\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 194ms/step - loss: 1.7771\n",
            "Epoch 3/4\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 195ms/step - loss: 1.6091\n",
            "Epoch 4/4\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 196ms/step - loss: 1.5329\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7de762cf6e30>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model with .keras extension\n",
        "model.save('textgenrator.keras')\n",
        "\n",
        "# Or, save the model with .h5 extension\n",
        "# model.save('textgenrator.h5')\n",
        "\n",
        "# Now, load the model using the correct format\n",
        "model = tf.keras.models.load_model('textgenrator.keras')\n",
        "\n",
        "# Or, if saved as .h5:\n",
        "# model = tf.keras.models.load_model('textgenrator.h5')"
      ],
      "metadata": {
        "id": "yvkOAGlG7pZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "metadata": {
        "id": "_arx7sQh7tLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(length, temperature):\n",
        "    start_index = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "    generated = ''\n",
        "    sentence = text[start_index: start_index + SEQ_LENGTH]\n",
        "    generated += sentence\n",
        "    for i in range(length):\n",
        "        x_predictions = np.zeros((1, SEQ_LENGTH, len(characters)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_predictions[0, t, char_to_index[char]] = 1\n",
        "\n",
        "        predictions = model.predict(x_predictions, verbose=0)[0]\n",
        "        next_index = sample(predictions,\n",
        "                                 temperature)\n",
        "        next_character = index_to_char[next_index]\n",
        "\n",
        "        generated += next_character\n",
        "        sentence = sentence[1:] + next_character\n",
        "    return generated\n"
      ],
      "metadata": {
        "id": "TXWHH-px7tOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(200, 0.2))\n",
        "print(generate_text(200, 0.4))\n",
        "print(generate_text(200, 0.6))\n",
        "print(generate_text(200, 0.8))\n",
        "print(generate_text(200, 1.0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYDfyDp_7tQQ",
        "outputId": "9de5f8b1-3528-4fad-c8b6-0d8db75422e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for honour,\n",
            "'tis a derivative from me to the took thee\n",
            "than the count thee to the power the prince,\n",
            "thou shall not the death of heart thee to thee\n",
            "that i have the soul the stand thee to the strenter\n",
            "than the some and the stand the world wit\n",
            "to the bending twigs.\n",
            "go thou, and like thee shall with as the foul father\n",
            "and man said the fier of with thy death?\n",
            "\n",
            "king henry vi:\n",
            "why were the entle to the more thee hands\n",
            "and with the death and thou art thou shall not doth his sorly thes\n",
            "hine and not king henry's heirs'\n",
            "\n",
            "york:\n",
            "my scrive a shain, our presence,\n",
            "the count thes shall may so that thou searening thy here,\n",
            "that the world my prove to thy ore is to here three,\n",
            "by these the more that and thee and now a party how us\n",
            "a\n",
            " good, my lord:\n",
            "richard not far from hence to that life:\n",
            "thouswelt than for no fier king of thy heady,\n",
            "good to chy of stay, like yese not fig rt now,\n",
            "which thou hast the prace of well is their bray;\n",
            "and daughter, remenge he out the found mo\n",
            "thee my free consent.\n",
            "\n",
            "warwick:\n",
            "and i chiefs!\n",
            "gvomy peare' scop we hereford of lift?\n",
            "\n",
            "duke of york:\n",
            "thy nobles he\n",
            "which with live time he of their sortelfire, provesardion:\n",
            "for not shame trey's true we slaight flow\n",
            "is fair d, warwarder: at \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ls_5J6F7tT5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}